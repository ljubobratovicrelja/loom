{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Loom","text":"<p>A lightweight visual pipeline runner for research.</p> <p>Connect your Python scripts into a graph, tweak parameters, run experiments, see results \u2014 without setting up Airflow or learning a workflow framework.</p> <ul> <li> <p> Try it now</p> <p>Open the live demo \u2014 no installation required. Browse and run example pipelines in your browser.</p> <p>First load may take ~30s to wake up.</p> </li> <li> <p> Install</p> <pre><code>pip install loom-pipeline[ui]\n</code></pre> <p>Add to any project's virtualenv and start building pipelines.</p> </li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>Loom gives you a CLI runner and visual editor for pipelines defined in YAML. Your scripts stay as regular Python with argparse \u2014 no framework to learn, no rewrites needed.</p> <p>It's designed for research workflows. For production orchestration, tools like Airflow or Kubeflow are better suited.</p>"},{"location":"#philosophy","title":"Philosophy","text":"<p>Loom is intentionally minimal:</p> <ul> <li>No database \u2014 Everything is files: your scripts, YAML configs, and outputs</li> <li>No external services \u2014 The visual editor runs a local server that stops when you close it</li> <li>No lock-in \u2014 Your scripts work with or without Loom</li> <li>No magic \u2014 Loom just builds shell commands and runs them</li> </ul> <p>This makes it easy to adopt incrementally. Start with one experiment, see if it helps, expand from there.</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># experiment.yml\nvariables:\n  video: data/raw/recording.mp4\n  features: data/processed/features.csv\n  model: models/classifier.pt\n\nparameters:\n  learning_rate: 0.001\n  epochs: 100\n\npipeline:\n  - name: extract\n    task: tasks/extract_features.py\n    inputs:\n      video: $video\n    outputs:\n      -o: $features\n\n  - name: train\n    task: tasks/train_model.py\n    inputs:\n      data: $features\n    outputs:\n      -o: $model\n    args:\n      --lr: $learning_rate\n      --epochs: $epochs\n</code></pre> <pre><code># Run the full pipeline\nloom experiment.yml\n\n# Or open in the visual editor\nloom-ui experiment.yml\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation \u2014 Get Loom installed</li> <li>Your First Pipeline \u2014 Run your first pipeline</li> <li>Tutorials \u2014 Learn through hands-on examples</li> </ul>"},{"location":"getting-started/first-pipeline/","title":"Your First Pipeline","text":"<p>Let's run an example pipeline to see Loom in action.</p>"},{"location":"getting-started/first-pipeline/#run-the-linear-example","title":"Run the Linear Example","text":"<p>Loom comes with example pipelines. Run the simplest one:</p> <pre><code>loom examples/linear/pipeline.yml\n</code></pre> <p>You should see output like:</p> <pre><code>Pipeline: 3 step(s) to run\n----------------------------------------\n[RUNNING] generate_data\n[generate_data] Generated 100 samples -&gt; data/readings.csv\n[SUCCESS] generate_data\n[RUNNING] compute_stats\n[compute_stats] Computed stats for 100 rows -&gt; data/stats.json\n[SUCCESS] compute_stats\n[RUNNING] format_report\n[format_report] Report written -&gt; data/report.txt\n[SUCCESS] format_report\n----------------------------------------\nCompleted: 3/3 steps succeeded\n</code></pre>"},{"location":"getting-started/first-pipeline/#what-just-happened","title":"What Just Happened?","text":"<p>The pipeline executed three steps in sequence:</p> <pre><code>generate_data \u2192 compute_stats \u2192 format_report\n</code></pre> <ol> <li>generate_data \u2014 Created a CSV with random sensor readings</li> <li>compute_stats \u2014 Calculated statistics (min, max, mean)</li> <li>format_report \u2014 Converted stats to a human-readable report</li> </ol>"},{"location":"getting-started/first-pipeline/#check-the-outputs","title":"Check the Outputs","text":"<pre><code>cat examples/linear/data/readings.csv   # Raw data\ncat examples/linear/data/stats.json     # Statistics\ncat examples/linear/data/report.txt     # Final report\n</code></pre>"},{"location":"getting-started/first-pipeline/#open-in-the-visual-editor","title":"Open in the Visual Editor","text":"<p>Now let's see the same pipeline visually:</p> <pre><code>loom-ui examples/linear/pipeline.yml\n</code></pre> <p>This opens a browser window where you can:</p> <ul> <li>See your pipeline as a visual graph</li> <li>Click nodes to view their properties</li> <li>Run individual steps by clicking the play button</li> <li>See output in the terminal panel at the bottom</li> </ul>"},{"location":"getting-started/first-pipeline/#useful-commands","title":"Useful Commands","text":"<pre><code># Preview without executing\nloom examples/linear/pipeline.yml --dry-run\n\n# Run just one step\nloom examples/linear/pipeline.yml --step compute_stats\n\n# Run from a step onward\nloom examples/linear/pipeline.yml --from compute_stats\n\n# Clean outputs and re-run\nloom examples/linear/pipeline.yml --clean -y\nloom examples/linear/pipeline.yml\n</code></pre>"},{"location":"getting-started/first-pipeline/#browse-all-examples","title":"Browse All Examples","text":"<p>Open the visual editor in workspace mode to browse all examples:</p> <pre><code>loom-ui examples/\n</code></pre> <p>A pipeline browser appears in the sidebar \u2014 double-click to switch between pipelines.</p>"},{"location":"getting-started/first-pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorials \u2014 Learn Loom concepts through hands-on examples</li> <li>CLI Commands \u2014 Full CLI reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11+</li> <li>Node.js 20+ (only for development/building from source)</li> </ul>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code># Core runner only\npip install loom-pipeline\n\n# With visual editor\npip install loom-pipeline[ui]\n</code></pre> <p>That's it. No configuration files to create, no external services to manage.</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Check CLI is available\nloom --help\n\n# Check UI is available (if installed with [ui])\nloom-ui --help\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing to Loom or running from source:</p> <pre><code>git clone https://github.com/ljubobratovicrelja/loom.git\ncd loom\npip install -e \".[dev]\"\n\n# Build frontend (requires Node.js 18+)\ncd src/loom/ui/frontend\nnpm install &amp;&amp; npm run build\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Your First Pipeline \u2014 Run an example pipeline</li> </ul>"},{"location":"reference/cli/","title":"CLI Commands","text":"<p>Reference for <code>loom</code> and <code>loom-ui</code> command-line tools.</p>"},{"location":"reference/cli/#loom","title":"loom","text":"<p>The headless pipeline runner for batch execution and automation.</p>"},{"location":"reference/cli/#basic-usage","title":"Basic Usage","text":"<pre><code># Run full pipeline\nloom pipeline.yml\n\n# Preview commands without executing\nloom pipeline.yml --dry-run\n</code></pre>"},{"location":"reference/cli/#command-reference","title":"Command Reference","text":"Option Description <code>--dry-run</code> Preview commands without executing <code>--step NAME [NAME ...]</code> Run specific step(s) only <code>--from NAME</code> Run from a step onward (includes all subsequent steps) <code>--include NAME [NAME ...]</code> Include optional step(s) <code>--set KEY=VALUE [...]</code> Override parameter values <code>--var KEY=VALUE [...]</code> Override variable values <code>--extra \"ARGS\"</code> Pass extra arguments to a step <code>--parallel</code> Enable parallel execution (overrides config) <code>--sequential</code> Force sequential execution (overrides config) <code>--max-workers N</code> Maximum parallel workers (default: CPU count) <code>--clean</code> Move all output data to trash <code>--clean-list</code> List files that would be cleaned (preview only) <code>--permanent</code> Permanently delete instead of trash (use with <code>--clean</code>) <code>-y, --yes</code> Skip confirmation prompts <code>--list</code> List all steps in the pipeline <code>--investigate</code> Show step interface (inputs/outputs/args); use with <code>--step</code>"},{"location":"reference/cli/#examples","title":"Examples","text":"<pre><code># Run full pipeline\nloom pipeline.yml\n\n# Preview without executing\nloom pipeline.yml --dry-run\n\n# Run specific steps\nloom pipeline.yml --step extract classify\n\n# Run from a step onward\nloom pipeline.yml --from classify\n\n# Include optional steps\nloom pipeline.yml --include debug_visualize\n\n# Override parameters\nloom pipeline.yml --set backend=local threshold=25.0\n\n# Override variables\nloom pipeline.yml --var video=other.mp4\n\n# Pass extra arguments to a step\nloom pipeline.yml --step extract --extra \"--debug --verbose\"\n\n# Run steps in parallel\nloom pipeline.yml --parallel\n\n# Run in parallel with limited workers\nloom pipeline.yml --parallel --max-workers 4\n\n# Force sequential even if config says parallel\nloom pipeline.yml --sequential\n\n# Clean all data and re-run\nloom pipeline.yml --clean -y\nloom pipeline.yml\n\n# Permanently delete instead of trash\nloom pipeline.yml --clean --permanent -y\n\n# Preview what would be cleaned\nloom pipeline.yml --clean-list\n\n# List all steps in a pipeline\nloom pipeline.yml --list\n\n# Inspect a step's interface (inputs, outputs, args)\nloom pipeline.yml --step compute_stats --investigate\n</code></pre>"},{"location":"reference/cli/#loom-ui","title":"loom-ui","text":"<p>The visual editor for building and running pipelines interactively.</p>"},{"location":"reference/cli/#basic-usage_1","title":"Basic Usage","text":"<pre><code># Open a pipeline\nloom-ui pipeline.yml\n\n# Browse pipelines in a directory\nloom-ui experiments/\n\n# Create a new pipeline\nloom-ui --new\n</code></pre>"},{"location":"reference/cli/#command-reference_1","title":"Command Reference","text":"Option Description <code>--port PORT</code> Server port (default: 8000) <code>--host HOST</code> Server host (default: 127.0.0.1) <code>--no-browser</code> Don't auto-open browser <code>--new</code> Create a new empty pipeline <code>--tasks-dir DIR</code> Custom tasks directory"},{"location":"reference/cli/#examples_1","title":"Examples","text":"<pre><code># Open existing pipeline\nloom-ui pipeline.yml\n\n# Browse all pipelines in a folder\nloom-ui experiments/\n\n# Custom port and host\nloom-ui pipeline.yml --port 8080 --host 0.0.0.0\n\n# Headless mode (no auto-open)\nloom-ui pipeline.yml --no-browser\n\n# Create new pipeline\nloom-ui --new\n</code></pre>"},{"location":"reference/cli/#workspace-mode","title":"Workspace Mode","text":"<p>When pointing <code>loom-ui</code> at a directory, it enters workspace mode:</p> <pre><code>loom-ui experiments/\n</code></pre> <p>A pipeline browser appears in the sidebar, letting you:</p> <ul> <li>Browse all <code>.yml</code> pipelines in the directory</li> <li>Double-click to switch between pipelines</li> <li>Get prompted to save unsaved changes before switching</li> </ul>"},{"location":"reference/cli/#environment","title":"Environment","text":"<p>Both tools require:</p> <ul> <li>Python 3.11+</li> </ul> <p>All relative paths in pipeline configs are resolved relative to the directory containing the YAML file, so your paths work correctly regardless of where you invoke <code>loom</code>.</p>"},{"location":"reference/editor/","title":"Visual Editor","text":"<p>Reference for the <code>loom-ui</code> visual editor.</p>"},{"location":"reference/editor/#starting-the-editor","title":"Starting the Editor","text":"<pre><code># Open a pipeline\nloom-ui pipeline.yml\n\n# Browse pipelines in a directory\nloom-ui experiments/\n\n# Create a new pipeline\nloom-ui --new\n\n# Custom port\nloom-ui pipeline.yml --port 8080\n</code></pre>"},{"location":"reference/editor/#user-interface","title":"User Interface","text":"<p>The editor has four main areas:</p> <ul> <li>Toolbar (top) \u2014 Logo, file path, run controls, save button</li> <li>Sidebar (left) \u2014 Add data nodes, browse tasks, manage parameters</li> <li>Canvas (center) \u2014 Visual graph of your pipeline</li> <li>Properties Panel (right) \u2014 Edit selected node, run individual steps</li> <li>Terminal Panel (bottom, collapsible) \u2014 Execution output</li> </ul>"},{"location":"reference/editor/#panels","title":"Panels","text":""},{"location":"reference/editor/#sidebar-left","title":"Sidebar (Left)","text":"<ul> <li>Data Types \u2014 Add new data nodes to the canvas</li> <li>Tasks \u2014 Click to add task steps to the canvas</li> <li>Parameters \u2014 Add/edit configuration values</li> </ul>"},{"location":"reference/editor/#canvas-center","title":"Canvas (Center)","text":"<ul> <li>Visual graph of your pipeline</li> <li>Drag nodes to arrange</li> <li>Connect steps to data nodes by dragging from handles</li> <li>Multi-select with Shift+click or drag box</li> <li>Mini-map in corner for navigation</li> </ul>"},{"location":"reference/editor/#properties-panel-right","title":"Properties Panel (Right)","text":"<ul> <li>Edit selected node properties</li> <li>For steps: name, task, inputs, outputs, args</li> <li>For data nodes: display name, key, type, path, description</li> <li>Run/Cancel buttons for individual steps</li> <li>Shows available data/parameter references</li> </ul>"},{"location":"reference/editor/#terminal-panel-bottom","title":"Terminal Panel (Bottom)","text":"<ul> <li>Collapsible output panel</li> <li>Live streaming execution output</li> <li>Per-step output tabs when running parallel</li> <li>ANSI color support</li> </ul>"},{"location":"reference/editor/#node-types","title":"Node Types","text":""},{"location":"reference/editor/#data-nodes","title":"Data Nodes","text":"<p>Represent file paths or URLs:</p> <ul> <li>Green = file exists on disk</li> <li>Grey = file doesn't exist yet</li> </ul>"},{"location":"reference/editor/#step-nodes","title":"Step Nodes","text":"<p>Represent Python tasks:</p> <ul> <li>Solid border = regular step</li> <li>Dashed border = optional step</li> <li>Optional and disabled checkboxes in properties panel</li> <li>Shows execution state (idle, running, completed, failed)</li> </ul>"},{"location":"reference/editor/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"Shortcut Action <code>Cmd/Ctrl + Z</code> Undo <code>Cmd/Ctrl + Shift + Z</code> Redo <code>Ctrl + Y</code> Redo (Windows) <code>Cmd/Ctrl + S</code> Save <code>Cmd/Ctrl + C</code> Copy selected nodes <code>Cmd/Ctrl + V</code> Paste nodes <code>Delete / Backspace</code> Delete selected"},{"location":"reference/editor/#running-pipelines","title":"Running Pipelines","text":""},{"location":"reference/editor/#run-controls-toolbar","title":"Run Controls (Toolbar)","text":"<ul> <li>Run All \u2014 Execute entire pipeline</li> <li>Stop \u2014 Cancel running execution</li> </ul>"},{"location":"reference/editor/#run-from-properties-panel","title":"Run from Properties Panel","text":"<p>Select a step to access:</p> <ul> <li>Run This Step \u2014 Execute just this step</li> </ul>"},{"location":"reference/editor/#run-from-toolbar","title":"Run from Toolbar","text":"<ul> <li>From Here \u2014 Execute from the selected step and all downstream</li> </ul>"},{"location":"reference/editor/#execution-states","title":"Execution States","text":"State Indicator Idle Default appearance Running Animated border Completed Green checkmark Failed Red indicator"},{"location":"reference/editor/#workspace-mode","title":"Workspace Mode","text":"<p>Open a directory to browse multiple pipelines:</p> <pre><code>loom-ui experiments/\n</code></pre> <p>Features:</p> <ul> <li>Pipeline browser in sidebar</li> <li>Double-click to switch pipelines</li> <li>Prompts to save unsaved changes before switching</li> </ul>"},{"location":"reference/editor/#connections","title":"Connections","text":""},{"location":"reference/editor/#creating-connections","title":"Creating Connections","text":"<ol> <li>Hover over a node handle (small circle)</li> <li>Drag to another node's handle</li> <li>Release to create connection</li> </ol>"},{"location":"reference/editor/#connection-rules","title":"Connection Rules","text":"From To Valid? Data node \u2192 Step input Yes Main data flow Step output \u2192 Data node Yes Step produces data Parameter \u2192 Step arg Yes Config value Data node \u2192 Data node No Not allowed"},{"location":"reference/editor/#type-validation","title":"Type Validation","text":"<p>When tasks have typed inputs/outputs, the editor validates that connected types match.</p>"},{"location":"reference/editor/#saving","title":"Saving","text":"<ul> <li>Auto-save is disabled by default</li> <li>Unsaved changes shown with <code>*</code> in title</li> <li>Cmd/Ctrl + S to save manually</li> <li>Prompted to save when switching pipelines or closing</li> </ul>"},{"location":"reference/editor/#tips","title":"Tips","text":"<ul> <li>Drag from handles to create connections</li> <li>Double-click data nodes to open the file path</li> <li>Shift+click to multi-select nodes</li> <li>Delete key removes selected nodes/connections</li> <li>Mini-map helps navigate large pipelines</li> </ul>"},{"location":"reference/pipeline-schema/","title":"Pipeline Schema","text":"<p>Complete reference for pipeline YAML configuration.</p>"},{"location":"reference/pipeline-schema/#overview","title":"Overview","text":"<p>A pipeline file has these sections:</p> <pre><code>data:        # File paths (simple strings or typed data nodes)\nparameters:  # Configuration values\npipeline:    # Processing steps\nexecution:   # Optional: parallel execution settings\n</code></pre>"},{"location":"reference/pipeline-schema/#data-section","title":"Data Section","text":"<p>The <code>data</code> section defines files in your pipeline. Each entry can be a simple path string or a typed data node.</p>"},{"location":"reference/pipeline-schema/#simple-paths","title":"Simple Paths","text":"<pre><code>data:\n  input_file: data/input.csv\n  output_file: data/output.csv\n</code></pre> <p>Reference with <code>$</code>: <code>$input_file</code> \u2192 <code>data/input.csv</code></p>"},{"location":"reference/pipeline-schema/#typed-data-nodes","title":"Typed Data Nodes","text":"<p>For better editor validation, use typed entries:</p> <pre><code>data:\n  training_video:\n    type: video\n    path: data/videos/training.mp4\n    description: Main training video\n\n  gaze_positions:\n    type: csv\n    path: data/tracking/gaze.csv\n    description: Extracted gaze coordinates\n</code></pre>"},{"location":"reference/pipeline-schema/#supported-types","title":"Supported Types","text":"Type Description Typical Extensions <code>video</code> Video file .mp4, .avi, .mov <code>image</code> Single image .png, .jpg, .jpeg <code>csv</code> CSV data file .csv <code>json</code> JSON data file .json <code>txt</code> Text file .txt <code>image_directory</code> Directory of images folder <code>data_folder</code> Generic data directory folder <p>Types enable connection validation in the visual editor.</p>"},{"location":"reference/pipeline-schema/#url-data-sources","title":"URL Data Sources","text":"<p>You can use HTTP/HTTPS URLs instead of local paths. URLs are automatically downloaded and cached locally.</p> <pre><code>data:\n  source_image:\n    type: image\n    path: https://example.com/images/photo.png\n    description: Image from URL\n</code></pre> <p>How it works:</p> <ol> <li>URLs are detected by <code>http://</code> or <code>https://</code> prefix</li> <li>On first access, the URL is downloaded to <code>.loom-url-cache/</code> in the pipeline directory</li> <li>Subsequent runs use the cached file (fast)</li> <li>Use <code>loom pipeline.yml --clean</code> to clear the cache and re-download</li> </ol> <p>Example:</p> <pre><code>data:\n  lena_image:\n    type: image\n    path: https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png\n    description: Lena test image\n</code></pre>"},{"location":"reference/pipeline-schema/#visual-representation","title":"Visual Representation","text":"<p>In the editor:</p> <ul> <li>Green = file exists on disk or URL is reachable</li> <li>Grey = file doesn't exist or URL is unreachable</li> <li>Link icon = path is a URL</li> </ul>"},{"location":"reference/pipeline-schema/#parameters-section","title":"Parameters Section","text":"<p>Parameters hold configuration values that can be shared across steps.</p> <pre><code>parameters:\n  # Numbers\n  threshold: 50.0\n  batch_size: 32\n\n  # Strings\n  model_name: \"gpt-4\"\n  output_format: csv\n\n  # Booleans\n  verbose: true\n  debug_mode: false\n</code></pre>"},{"location":"reference/pipeline-schema/#using-parameters","title":"Using Parameters","text":"<p>Reference with <code>$</code> in the <code>args</code> section:</p> <pre><code>pipeline:\n  - name: process\n    args:\n      --threshold: $threshold  # Becomes --threshold 50.0\n      --verbose: $verbose      # Becomes --verbose (if true)\n</code></pre>"},{"location":"reference/pipeline-schema/#runtime-overrides","title":"Runtime Overrides","text":"<p>Override parameters from the command line:</p> <pre><code>loom pipeline.yml --set threshold=25.0 batch_size=64\n</code></pre>"},{"location":"reference/pipeline-schema/#pipeline-section","title":"Pipeline Section","text":"<p>The <code>pipeline</code> section defines processing steps.</p>"},{"location":"reference/pipeline-schema/#step-fields","title":"Step Fields","text":"Field Required Description <code>name</code> Yes Unique identifier for the step <code>task</code> Yes Path to the Python script <code>inputs</code> No Named inputs mapped to data entries <code>outputs</code> No Output flags mapped to data entries <code>args</code> No Additional command-line arguments <code>optional</code> No If <code>true</code>, skipped unless <code>--include</code>d"},{"location":"reference/pipeline-schema/#basic-step","title":"Basic Step","text":"<pre><code>pipeline:\n  - name: process_data\n    task: tasks/process.py\n    inputs:\n      data: $input_file\n    outputs:\n      --output: $output_file\n</code></pre>"},{"location":"reference/pipeline-schema/#step-with-arguments","title":"Step with Arguments","text":"<pre><code>pipeline:\n  - name: train_model\n    task: tasks/train.py\n    inputs:\n      data: $training_data\n    outputs:\n      --model: $model_file\n    args:\n      --epochs: 100\n      --learning-rate: $learning_rate\n      --verbose: true\n</code></pre>"},{"location":"reference/pipeline-schema/#optional-step","title":"Optional Step","text":"<pre><code>pipeline:\n  - name: visualize\n    task: tasks/visualize.py\n    optional: true  # Skipped unless --include visualize\n    inputs:\n      data: $results\n    outputs:\n      --output: $chart\n</code></pre> <p>Run with: <code>loom pipeline.yml --include visualize</code></p>"},{"location":"reference/pipeline-schema/#group-block","title":"Group Block","text":"<p>Group related steps visually in the editor by wrapping them in a <code>group:</code> block:</p> <pre><code>pipeline:\n  - group: preprocessing\n    steps:\n      - name: preprocess\n        task: tasks/preprocess.py\n        outputs:\n          --output: $clean_data\n\n      - name: normalize\n        task: tasks/normalize.py\n        inputs:\n          data: $clean_data\n        outputs:\n          --output: $normalized_data\n\n  - name: train\n    task: tasks/train.py\n    inputs:\n      data: $normalized_data\n</code></pre> <p>Groups are purely visual \u2014 they don't affect execution order, dependency resolution, or parallelism. In loom-ui, each group is drawn as a colored rectangle behind its member nodes.</p> <p>Grouped and ungrouped steps can be mixed freely in the same pipeline.</p>"},{"location":"reference/pipeline-schema/#command-generation","title":"Command Generation","text":"<p>Steps become shell commands:</p> <pre><code>- name: detect_fixations\n  task: tasks/detect_fixations.py\n  inputs:\n    gaze_csv: $gaze_positions\n  outputs:\n    -o: $fixations_csv\n  args:\n    --algorithm: ivt\n    --threshold: $velocity_threshold\n</code></pre> <p>Becomes:</p> <pre><code>python tasks/detect_fixations.py data/gaze.csv -o data/fixations.csv --algorithm ivt --threshold 50.0\n</code></pre>"},{"location":"reference/pipeline-schema/#argument-order","title":"Argument Order","text":"<ol> <li>Inputs \u2014 positional arguments in order listed</li> <li>Outputs \u2014 flag arguments (e.g., <code>-o value</code>)</li> <li>Args \u2014 additional arguments</li> </ol>"},{"location":"reference/pipeline-schema/#execution-section","title":"Execution Section","text":"<p>Configure how the pipeline runs:</p> <pre><code>execution:\n  parallel: true      # Enable parallel execution\n  max_workers: 4      # Maximum concurrent steps (default: CPU count)\n</code></pre> Field Default Description <code>parallel</code> <code>false</code> Enable parallel step execution <code>max_workers</code> CPU count Maximum concurrent workers <p>Override from command line:</p> <pre><code>loom pipeline.yml --parallel --max-workers 2\nloom pipeline.yml --sequential  # Force sequential\n</code></pre>"},{"location":"reference/pipeline-schema/#execution-order","title":"Execution Order","text":"<p>Loom determines execution order from dependencies:</p> <ol> <li>Steps with no input dependencies run first</li> <li>A step runs after all steps producing its inputs complete</li> <li>Independent steps can run in parallel (if enabled)</li> <li>Optional steps are skipped unless explicitly included</li> </ol>"},{"location":"reference/pipeline-schema/#complete-example","title":"Complete Example","text":"<pre><code>data:\n  # Inputs\n  source_video:\n    type: video\n    path: data/raw/video.mp4\n\n  # Intermediates\n  gaze_csv:\n    type: csv\n    path: data/processed/gaze.csv\n\n  fixations_csv:\n    type: csv\n    path: data/processed/fixations.csv\n\n  # Outputs\n  final_report:\n    type: json\n    path: data/output/report.json\n\n  debug_video:\n    type: video\n    path: data/output/debug.mp4\n\nparameters:\n  threshold: 50.0\n  algorithm: ivt\n  debug: false\n\npipeline:\n  - name: extract_gaze\n    task: tasks/extract_gaze.py\n    inputs:\n      video: $source_video\n    outputs:\n      -o: $gaze_csv\n\n  - name: detect_fixations\n    task: tasks/detect_fixations.py\n    inputs:\n      gaze: $gaze_csv\n    outputs:\n      -o: $fixations_csv\n    args:\n      --algorithm: $algorithm\n      --threshold: $threshold\n\n  - name: generate_report\n    task: tasks/report.py\n    inputs:\n      fixations: $fixations_csv\n    outputs:\n      -o: $final_report\n\n  - name: visualize\n    task: tasks/visualize.py\n    optional: true\n    inputs:\n      video: $source_video\n      fixations: $fixations_csv\n    outputs:\n      -o: $debug_video\n</code></pre>"},{"location":"reference/task-scripts/","title":"Task Scripts","text":"<p>How to write Python scripts that work with Loom pipelines.</p>"},{"location":"reference/task-scripts/#requirements","title":"Requirements","text":"<p>For a Python script to work as a pipeline task:</p> <ol> <li>Use argparse for command-line arguments</li> <li>Add YAML frontmatter in the docstring (optional but recommended)</li> <li>Match argument names between frontmatter and argparse</li> </ol>"},{"location":"reference/task-scripts/#basic-structure","title":"Basic Structure","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Short description of what this task does.\n\nLonger description with details about the algorithm,\nexpected inputs, and outputs.\n\n---\ninputs:\n  data:\n    type: csv\n    description: Input data file\noutputs:\n  -o:\n    type: json\n    description: Output results file\nargs:\n  --threshold:\n    type: float\n    default: 0.5\n    description: Detection threshold\n---\n\"\"\"\n\nimport argparse\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    # Positional input (matches 'data' in frontmatter)\n    parser.add_argument('data', help='Input data file')\n\n    # Output flag (matches '-o' in frontmatter)\n    parser.add_argument('-o', '--output', required=True, help='Output file')\n\n    # Optional argument (matches '--threshold' in frontmatter)\n    parser.add_argument('--threshold', type=float, default=0.5)\n\n    args = parser.parse_args()\n\n    # Your processing logic here\n    # ...\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"reference/task-scripts/#frontmatter-schema","title":"Frontmatter Schema","text":"<p>The YAML block between <code>---</code> markers defines the task interface.</p>"},{"location":"reference/task-scripts/#inputs","title":"Inputs","text":"<p>Positional arguments that receive file paths:</p> <pre><code>inputs:\n  video:\n    type: video\n    description: Path to input video file\n  gaze_csv:\n    type: csv\n    description: Path to gaze positions CSV\n</code></pre> <p>In argparse:</p> <pre><code>parser.add_argument('video', help='Path to input video')\nparser.add_argument('gaze_csv', help='Path to gaze CSV')\n</code></pre>"},{"location":"reference/task-scripts/#outputs","title":"Outputs","text":"<p>Flag arguments for output file paths:</p> <pre><code>outputs:\n  -o:\n    type: csv\n    description: Output CSV file\n  --model:\n    type: file\n    description: Trained model file\n</code></pre> <p>In argparse:</p> <pre><code>parser.add_argument('-o', '--output', required=True)\nparser.add_argument('--model', required=True)\n</code></pre>"},{"location":"reference/task-scripts/#arguments","title":"Arguments","text":"<p>Additional configuration flags:</p> <pre><code>args:\n  --algorithm:\n    type: str\n    default: ivt\n    choices: [ivt, idt]\n    description: Detection algorithm\n  --threshold:\n    type: float\n    default: 50.0\n    description: Velocity threshold\n  --verbose:\n    type: bool\n    description: Enable verbose output\n</code></pre> <p>In argparse:</p> <pre><code>parser.add_argument('--algorithm', default='ivt', choices=['ivt', 'idt'])\nparser.add_argument('--threshold', type=float, default=50.0)\nparser.add_argument('--verbose', action='store_true')\n</code></pre>"},{"location":"reference/task-scripts/#field-reference","title":"Field Reference","text":""},{"location":"reference/task-scripts/#inputoutput-fields","title":"Input/Output Fields","text":"Field Required Description <code>type</code> No Data type for validation <code>description</code> No Help text shown in UI"},{"location":"reference/task-scripts/#argument-fields","title":"Argument Fields","text":"Field Required Description <code>type</code> No <code>str</code>, <code>int</code>, <code>float</code>, or <code>bool</code> <code>default</code> No Default value <code>description</code> No Help text <code>choices</code> No List of valid options <code>required</code> No Whether argument is required (default: false)"},{"location":"reference/task-scripts/#supported-types","title":"Supported Types","text":"<p>For inputs/outputs (enables connection validation):</p> <ul> <li><code>video</code>, <code>image</code>, <code>csv</code>, <code>json</code>, <code>txt</code></li> <li><code>image_directory</code>, <code>data_folder</code></li> </ul> <p>For arguments:</p> <ul> <li><code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code></li> </ul>"},{"location":"reference/task-scripts/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Detect fixation events in gaze data.\n\nUses the I-VT (velocity threshold) or I-DT (dispersion threshold)\nalgorithm to identify fixation events from raw gaze positions.\n\n---\ninputs:\n  gaze_csv:\n    type: csv\n    description: Path to gaze positions CSV (x, y, timestamp)\noutputs:\n  -o:\n    type: csv\n    description: Output CSV with detected fixations\nargs:\n  --algorithm:\n    type: str\n    default: ivt\n    choices: [ivt, idt]\n    description: Detection algorithm to use\n  --threshold:\n    type: float\n    default: 50.0\n    description: Velocity threshold (px/frame) for I-VT\n  --min-duration:\n    type: float\n    default: 0.1\n    description: Minimum fixation duration (seconds)\n  --verbose:\n    type: bool\n    description: Print progress information\n---\n\"\"\"\n\nimport argparse\nimport csv\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Detect fixation events in gaze data'\n    )\n\n    # Input (positional)\n    parser.add_argument('gaze_csv', help='Path to gaze positions CSV')\n\n    # Output (flag)\n    parser.add_argument('-o', '--output', required=True,\n                        help='Output CSV path')\n\n    # Optional arguments\n    parser.add_argument('--algorithm', default='ivt',\n                        choices=['ivt', 'idt'],\n                        help='Detection algorithm')\n    parser.add_argument('--threshold', type=float, default=50.0,\n                        help='Velocity threshold')\n    parser.add_argument('--min-duration', type=float, default=0.1,\n                        help='Minimum fixation duration')\n    parser.add_argument('--verbose', action='store_true',\n                        help='Print progress')\n\n    args = parser.parse_args()\n\n    if args.verbose:\n        print(f'Processing {args.gaze_csv}')\n        print(f'Algorithm: {args.algorithm}, threshold: {args.threshold}')\n\n    # Load gaze data\n    with open(args.gaze_csv) as f:\n        reader = csv.DictReader(f)\n        gaze_data = list(reader)\n\n    # Detect fixations (simplified)\n    fixations = detect_fixations(\n        gaze_data,\n        algorithm=args.algorithm,\n        threshold=args.threshold,\n        min_duration=args.min_duration\n    )\n\n    # Write output\n    with open(args.output, 'w', newline='') as f:\n        writer = csv.DictWriter(f, fieldnames=['start', 'end', 'x', 'y'])\n        writer.writeheader()\n        writer.writerows(fixations)\n\n    if args.verbose:\n        print(f'Found {len(fixations)} fixations')\n        print(f'Written to {args.output}')\n\n\ndef detect_fixations(data, algorithm, threshold, min_duration):\n    # Your detection logic here\n    return []\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"reference/task-scripts/#pipeline-usage","title":"Pipeline Usage","text":"<p>The script above would be used in a pipeline like:</p> <pre><code>pipeline:\n  - name: detect_fixations\n    task: tasks/detect_fixations.py\n    inputs:\n      gaze_csv: $gaze_positions\n    outputs:\n      -o: $fixations_csv\n    args:\n      --algorithm: ivt\n      --threshold: $velocity_threshold\n      --verbose: true\n</code></pre> <p>Which generates:</p> <pre><code>python tasks/detect_fixations.py data/gaze.csv -o data/fixations.csv \\\n    --algorithm ivt --threshold 50.0 --verbose\n</code></pre>"},{"location":"reference/task-scripts/#tips","title":"Tips","text":"<ul> <li>Keep scripts standalone \u2014 They should work with or without Loom</li> <li>Use argparse \u2014 It's the standard and Loom parses it reliably</li> <li>Add frontmatter \u2014 It enables validation and better UI in the editor</li> <li>Match names \u2014 Frontmatter input/arg names must match argparse argument names</li> <li>Use types \u2014 Type hints enable connection validation in the editor</li> <li>Inspect from CLI \u2014 Run <code>loom pipeline.yml --step NAME --investigate</code> to preview a step's interface (inputs, outputs, args) without opening the editor</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Learn Loom through hands-on examples. Each tutorial builds on the previous one, introducing new concepts progressively.</p>"},{"location":"tutorials/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have Loom installed and have run your first pipeline.</p>"},{"location":"tutorials/#tutorial-progression","title":"Tutorial Progression","text":"Tutorial What You'll Learn Linear Pipeline Core concepts: data nodes, steps, inputs/outputs Branching (Diamond) Branching, merging, multiple inputs Using Parameters Config values and <code>--set</code> overrides Parallel Execution Same task with different configs Optional Steps Conditional execution with <code>--include</code> Scientific Workflow A complete scientific workflow example Image Processing Working with images and URL caching Loop (Per-Item Processing) Iterating over every file in a directory Groups Visual grouping of steps in the editor"},{"location":"tutorials/#running-examples","title":"Running Examples","text":"<p>Each tutorial uses a self-contained example from the <code>examples/</code> directory:</p> <pre><code># Run any example\nloom examples/&lt;name&gt;/pipeline.yml\n\n# Open in visual editor\nloom-ui examples/&lt;name&gt;/pipeline.yml\n\n# Browse all examples\nloom-ui examples/\n</code></pre>"},{"location":"tutorials/#start-here","title":"Start Here","text":"<p>Linear Pipeline \u2192</p>"},{"location":"tutorials/curve-fitting/","title":"Curve Fitting Example","text":"<p>This example demonstrates non-linear curve fitting with visualization steps.</p>"},{"location":"tutorials/curve-fitting/#overview","title":"Overview","text":"<p>The pipeline: 1. Generates synthetic data from a known exponential decay function with added Gaussian noise 2. Plots the raw data as a scatter plot 3. Fits an exponential model using scipy's curve_fit 4. Creates a comparison plot showing data points with the fitted curve</p>"},{"location":"tutorials/curve-fitting/#model","title":"Model","text":"<p>The underlying model is exponential decay:</p> <pre><code>y = a * exp(-b * x) + c\n</code></pre> <p>True parameters used for data generation: <code>a=5.0, b=0.3, c=1.0</code></p>"},{"location":"tutorials/curve-fitting/#pipeline-structure","title":"Pipeline Structure","text":"<pre><code>generate_data \u2500\u2500\u25ba raw_data.csv \u2500\u2500\u252c\u2500\u2500\u25ba plot_raw \u2500\u2500\u25ba raw_plot.png\n                                 \u2502\n                                 \u251c\u2500\u2500\u25ba fit_curve \u2500\u2500\u25ba fit_params.json\n                                 \u2502                        \u2502\n                                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u25ba plot_fit \u2500\u2500\u25ba fit_plot.png\n</code></pre>"},{"location":"tutorials/curve-fitting/#running","title":"Running","text":"<pre><code># Install example dependencies\npip install loom-pipeline[examples]\n\n# Run the pipeline\nloom examples/curve-fitting/pipeline.yml\n\n# Or open in the visual editor\nloom-ui examples/curve-fitting/pipeline.yml\n</code></pre>"},{"location":"tutorials/curve-fitting/#parameters","title":"Parameters","text":"<ul> <li><code>num_samples</code>: Number of data points (default: 50)</li> <li><code>noise_level</code>: Standard deviation of Gaussian noise (default: 0.5)</li> </ul>"},{"location":"tutorials/curve-fitting/#dependencies","title":"Dependencies","text":"<ul> <li>numpy</li> <li>scipy</li> <li>matplotlib</li> </ul>"},{"location":"tutorials/diamond/","title":"Diamond Pipeline","text":"<p>Branching and merging: one input feeds two parallel processing paths that combine at the end.</p> <pre><code>                \u250c\u2500\u25ba compute_stats \u2500\u2500\u25ba stats.json \u2500\u2500\u2500\u2510\nload_data \u2500\u2500\u2500\u2500\u2500\u2500\u2524                                   \u251c\u2500\u2500\u25ba merge_results\n                \u2514\u2500\u25ba filter_outliers \u2500\u2500\u25ba clean.csv \u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/diamond/#what-it-does","title":"What It Does","text":"<ol> <li>load_data: Reads raw measurements and validates the format</li> <li>compute_stats: Calculates statistics on the raw data (parallel path A)</li> <li>filter_outliers: Removes outliers from the data (parallel path B)</li> <li>merge_results: Combines stats with cleaned data into a final report</li> </ol> <p>This pattern is common when you need to both analyze and clean data, then combine the results.</p>"},{"location":"tutorials/diamond/#run-it","title":"Run It","text":"<pre><code># Full pipeline (parallel steps run in dependency order)\nloom examples/diamond/pipeline.yml\n\n# Check the outputs\ncat examples/diamond/data/raw.csv           # Input data\ncat examples/diamond/data/validated.csv     # After validation\ncat examples/diamond/data/stats.json        # Statistics (path A)\ncat examples/diamond/data/clean.csv         # Outliers removed (path B)\ncat examples/diamond/data/final_report.json # Combined results\n\n# Run from a specific step\nloom examples/diamond/pipeline.yml --from filter_outliers\n\n# Open in editor to see the diamond shape\nloom-ui examples/diamond/pipeline.yml\n</code></pre>"},{"location":"tutorials/diamond/#files","title":"Files","text":"<ul> <li><code>pipeline.yml</code> \u2014 Pipeline configuration</li> <li><code>tasks/load_data.py</code> \u2014 Validates and loads raw CSV</li> <li><code>tasks/compute_stats.py</code> \u2014 Computes statistics</li> <li><code>tasks/filter_outliers.py</code> \u2014 Removes outliers using IQR method</li> <li><code>tasks/merge_results.py</code> \u2014 Combines stats and clean data</li> <li><code>data/raw.csv</code> \u2014 Input data with some outliers</li> </ul>"},{"location":"tutorials/groups/","title":"Groups (Visual Step Grouping)","text":"<p>Organize related steps into named groups for a cleaner visual layout in the editor.</p> <pre><code>[ ingestion          ]\n[ generate \u2500\u25ba load   ] \u2500\u2500\u25ba compute_stats \u2500\u2510\n                       \u2514\u2500\u2500\u25ba filter        \u2500\u2534\u2500\u2500\u25ba merge\n[ analysis           ]\n</code></pre>"},{"location":"tutorials/groups/#what-it-does","title":"What It Does","text":"<p>Groups are a purely visual/organizational feature \u2014 they don't affect execution order or logic. Steps inside a <code>group:</code> block behave identically to ungrouped steps, but in loom-ui they're drawn inside a colored rectangle so you can see which steps belong together at a glance.</p> <p>This example takes the diamond pipeline and wraps its steps into two groups:</p> <ol> <li>ingestion: <code>generate_data</code> and <code>load_data</code> \u2014 the data acquisition phase.</li> <li>analysis: <code>compute_stats</code> and <code>filter_outliers</code> \u2014 the parallel analysis branches.</li> <li><code>merge_results</code> is left ungrouped as the final reporting step.</li> </ol>"},{"location":"tutorials/groups/#run-it","title":"Run It","text":"<pre><code># Run from the command line (works identically to a flat pipeline)\nloom examples/groups/pipeline.yml\n\n# Open in the visual editor to see the group rectangles\nloom-ui examples/groups/pipeline.yml\n</code></pre>"},{"location":"tutorials/groups/#the-group-block","title":"The Group Block","text":"<p>The <code>group:</code> block wraps one or more steps inside a named block:</p> <pre><code>pipeline:\n  - group: ingestion\n    steps:\n      - name: generate_data\n        task: tasks/generate_data.py\n        outputs:\n          --output: $raw_csv\n        args:\n          --rows: $num_rows\n\n      - name: load_data\n        task: tasks/load_data.py\n        inputs:\n          raw: $raw_csv\n        outputs:\n          --output: $validated_csv\n\n  - group: analysis\n    steps:\n      - name: compute_stats\n        task: tasks/compute_stats.py\n        inputs:\n          data: $validated_csv\n        outputs:\n          --output: $stats_json\n\n      - name: filter_outliers\n        task: tasks/filter_outliers.py\n        inputs:\n          data: $validated_csv\n        outputs:\n          --output: $clean_csv\n        args:\n          --threshold: $outlier_threshold\n\n  # Ungrouped steps still work\n  - name: merge_results\n    task: tasks/merge_results.py\n    inputs:\n      stats: $stats_json\n      clean_data: $clean_csv\n    outputs:\n      --output: $final_report\n</code></pre>"},{"location":"tutorials/groups/#key-points","title":"Key Points","text":"<ul> <li>Groups are YAML-only \u2014 there's no UI interaction for creating or modifying them.   Edit the YAML to add/change groups.</li> <li>Groups are visual-only \u2014 they don't change execution order, dependency resolution,   or parallelism.</li> <li>In loom-ui, each group gets a semi-transparent colored rectangle drawn behind its member   nodes, with a label that stays the same screen size regardless of zoom level.</li> <li>The Auto Layout button clusters grouped steps together using Dagre's compound graph   feature.</li> <li>Ungrouped steps and grouped steps can be freely mixed in the same pipeline.</li> </ul>"},{"location":"tutorials/groups/#when-to-use-groups","title":"When to Use Groups","text":"<ul> <li>Pipelines with 5+ steps where logical phases become hard to see.</li> <li>Separating data acquisition, processing, and reporting phases.</li> <li>Marking experimental branches or optional sub-workflows.</li> </ul>"},{"location":"tutorials/groups/#files","title":"Files","text":"<ul> <li><code>pipeline.yml</code> \u2014 Pipeline with <code>group:</code> blocks</li> <li><code>tasks/</code> \u2014 Same task scripts as the diamond example</li> </ul>"},{"location":"tutorials/image-processing/","title":"Image Processing Pipeline","text":"<p>This example demonstrates URL data sources and basic image processing with OpenCV.</p>"},{"location":"tutorials/image-processing/#what-this-example-shows","title":"What This Example Shows","text":"<ol> <li>URL as Data Source - The source image is loaded directly from a URL (Wikipedia's Lenna test image)</li> <li>Automatic Caching - URL resources are downloaded and cached in <code>.loom-url-cache/</code></li> <li>Branching Pipeline - Grayscale output feeds into two parallel processing steps</li> <li>Image Thumbnails - The visual editor shows thumbnails for each image</li> </ol>"},{"location":"tutorials/image-processing/#pipeline-structure","title":"Pipeline Structure","text":"<pre><code>[URL: Lena image]\n        |\n        v\n   [grayscale]\n     /      \\\n    v        v\n[edge_detect]  [blur]\n    |            |\n    v            v\nedges.png   blurred.png\n</code></pre>"},{"location":"tutorials/image-processing/#running-the-example","title":"Running the Example","text":"<pre><code># Navigate to the example directory\ncd examples/image-processing\n\n# Run the pipeline\nloom examples/image-processing/pipeline.yml\n\n# Or run the full pipeline\nloom examples/image-processing/pipeline.yml --all\n</code></pre> <p>First run will download the image from Wikipedia and cache it locally. Subsequent runs use the cached version.</p>"},{"location":"tutorials/image-processing/#tasks","title":"Tasks","text":"Task Description Input Output <code>grayscale.py</code> Convert to grayscale Color image Grayscale image <code>edge_detect.py</code> Canny edge detection Grayscale image Edge image <code>blur.py</code> Gaussian blur Any image Blurred image"},{"location":"tutorials/image-processing/#parameters","title":"Parameters","text":"Parameter Default Description <code>blur_radius</code> 15 Blur kernel size (pixels) <code>edge_threshold_low</code> 50 Canny low threshold <code>edge_threshold_high</code> 150 Canny high threshold"},{"location":"tutorials/image-processing/#clearing-the-url-cache","title":"Clearing the URL Cache","text":"<p>To re-download the source image:</p> <pre><code>loom clean pipeline.yml\n</code></pre> <p>This removes both the generated outputs and the URL cache directory.</p>"},{"location":"tutorials/image-processing/#dependencies","title":"Dependencies","text":"<ul> <li>OpenCV (<code>opencv-python-headless</code> or <code>opencv-python</code>)</li> </ul> <p>Install with: <pre><code>pip install opencv-python-headless\n</code></pre></p> <p>Or install all example dependencies: <pre><code>pip install -e \".[examples]\"\n</code></pre></p>"},{"location":"tutorials/linear/","title":"Linear Pipeline","text":"<p>The simplest pipeline pattern: A \u2192 B \u2192 C.</p> <pre><code>generate_data \u2192 compute_stats \u2192 format_report\n</code></pre>"},{"location":"tutorials/linear/#what-it-does","title":"What It Does","text":"<ol> <li>generate_data: Creates a CSV with random sensor readings (timestamp, temperature, humidity)</li> <li>compute_stats: Calculates min, max, mean for each column</li> <li>format_report: Converts stats to a human-readable text report</li> </ol>"},{"location":"tutorials/linear/#run-it","title":"Run It","text":"<pre><code># Full pipeline\nloom examples/linear/pipeline.yml\n\n# Check the outputs\ncat examples/linear/data/readings.csv      # Raw generated data\ncat examples/linear/data/stats.json        # Computed statistics\ncat examples/linear/data/report.txt        # Final report\n\n# Run just one step\nloom examples/linear/pipeline.yml --step compute_stats\n\n# Open in editor\nloom-ui examples/linear/pipeline.yml\n</code></pre>"},{"location":"tutorials/linear/#files","title":"Files","text":"<ul> <li><code>pipeline.yml</code> \u2014 Pipeline configuration</li> <li><code>tasks/generate_data.py</code> \u2014 Generates synthetic sensor data</li> <li><code>tasks/compute_stats.py</code> \u2014 Computes statistics from CSV</li> <li><code>tasks/format_report.py</code> \u2014 Formats stats as text report</li> <li><code>data/</code> \u2014 Input/output directory</li> </ul>"},{"location":"tutorials/loop/","title":"Loop Pipeline (Iterate Over a Collection)","text":"<p>Running the same task for every file in a directory \u2014 without writing a single loop in Python.</p> <pre><code>input_texts/ \u2500\u2500\u25ba uppercase_each (\u2200 item) \u2500\u2500\u25ba processed_texts/ \u2500\u2500\u25ba summarize \u2500\u2500\u25ba summary.txt\n</code></pre>"},{"location":"tutorials/loop/#what-it-does","title":"What It Does","text":"<p>This example introduces the <code>loop:</code> block \u2014 a first-class primitive for iterating over every file in a data folder.</p> <ol> <li>uppercase_each: Loops over every <code>.txt</code> file in <code>data/input/</code>, uppercases its    contents, and writes each result to <code>data/output/</code> (preserving filename).</li> <li>summarize: Reads the whole <code>data/output/</code> folder and concatenates all files into    a single <code>data/summary.txt</code>.</li> </ol> <p>The loop is declared entirely in YAML \u2014 no wrapper script needed.</p>"},{"location":"tutorials/loop/#run-it","title":"Run It","text":"<pre><code># Run the pipeline\nloom examples/loop/pipeline.yml\n\n# Check per-item outputs\nls examples/loop/data/output/\ncat examples/loop/data/output/foo.txt\n\n# Check the aggregated summary\ncat examples/loop/data/summary.txt\n\n# Open in the visual editor\nloom-ui examples/loop/pipeline.yml\n</code></pre>"},{"location":"tutorials/loop/#the-loop-block","title":"The Loop Block","text":"<p>The key addition is the <code>loop:</code> block on a step:</p> <pre><code>- name: uppercase_each\n  task: tasks/uppercase.py\n  loop:\n    over: $input_texts     # data_folder to iterate over\n    into: $processed_texts # data_folder where per-item outputs land\n    filter: \"*.txt\"        # optional glob filter\n  inputs:\n    input: $loop_item      # reserved: path of the current file\n  outputs:\n    --output: $loop_output # reserved: corresponding path in `into`\n  args:\n    --prefix: $prefix\n</code></pre> <p>Two reserved variables are available inside a loop step:</p> Variable Meaning <code>$loop_item</code> Absolute path of the current file being processed <code>$loop_output</code> Corresponding output path in the <code>into</code> folder (same filename) <p>Downstream steps consume the <code>into</code> folder exactly like any other data node \u2014 they don't need to know it was populated by a loop.</p>"},{"location":"tutorials/loop/#pattern-per-item-processing","title":"Pattern: Per-Item Processing","text":"<p>Use <code>loop:</code> whenever a step should run once per file:</p> <ul> <li>Resize every image in a directory</li> <li>Extract features from each audio clip</li> <li>Run inference on each sample in a dataset</li> </ul> <p>To run iterations concurrently, add <code>parallel: true</code> to the loop block.</p>"},{"location":"tutorials/loop/#files","title":"Files","text":"<ul> <li><code>pipeline.yml</code> \u2014 Loop pipeline definition</li> <li><code>tasks/uppercase.py</code> \u2014 Reads a text file, writes uppercased version</li> <li><code>tasks/summarize.py</code> \u2014 Concatenates a folder of files into one summary</li> <li><code>data/input/</code> \u2014 Sample text files (<code>foo.txt</code>, <code>greet.txt</code>, <code>hello.txt</code>)</li> </ul>"},{"location":"tutorials/optional-steps/","title":"Optional Steps Example","text":"<p>Using optional steps for debug output and visualization that you don't always need.</p>"},{"location":"tutorials/optional-steps/#what-it-does","title":"What It Does","text":"<p>A text processing pipeline that: 1. load_text: Reads input text file 2. word_frequency: Counts word occurrences 3. top_words: Extracts most common words 4. export_debug (optional): Dumps full frequency data for debugging 5. visualize (optional): Creates ASCII bar chart of top words</p> <p>Optional steps are skipped by default but can be included with <code>--include</code>.</p>"},{"location":"tutorials/optional-steps/#run-it","title":"Run It","text":"<pre><code># Run main pipeline (skips optional steps)\nloom examples/optional_steps/pipeline.yml\n\n# Check outputs\ncat examples/optional_steps/data/frequencies.json    # Word counts\ncat examples/optional_steps/data/top_words.json      # Top 10 words\n\n# Include the debug export\nloom examples/optional_steps/pipeline.yml --include export_debug\ncat examples/optional_steps/data/debug_dump.txt\n\n# Include visualization\nloom examples/optional_steps/pipeline.yml --include visualize\ncat examples/optional_steps/data/chart.txt\n\n# Include both optional steps\nloom examples/optional_steps/pipeline.yml --include export_debug --include visualize\n\n# Open in editor (optional steps shown with dashed borders)\nloom-ui examples/optional_steps/pipeline.yml\n</code></pre>"},{"location":"tutorials/optional-steps/#files","title":"Files","text":"<ul> <li><code>pipeline.yml</code> \u2014 Pipeline with optional steps marked</li> <li><code>tasks/load_text.py</code> \u2014 Loads and normalizes text</li> <li><code>tasks/word_frequency.py</code> \u2014 Counts word occurrences</li> <li><code>tasks/top_words.py</code> \u2014 Extracts top N words</li> <li><code>tasks/export_debug.py</code> \u2014 (Optional) Full debug dump</li> <li><code>tasks/visualize.py</code> \u2014 (Optional) ASCII bar chart</li> <li><code>data/sample.txt</code> \u2014 Sample input text</li> </ul>"},{"location":"tutorials/parallel/","title":"Parallel Pipeline (Hyperparameter Search)","text":"<p>Running the same processing with different parameter configurations to compare results.</p> <pre><code>                    \u250c\u2500\u25ba process (config A) \u2500\u25ba results_a.json \u2500\u2500\u2510\ngenerate_data \u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u25ba process (config B) \u2500\u25ba results_b.json \u2500\u2500\u253c\u2500\u25ba compare_results\n                    \u2514\u2500\u25ba process (config C) \u2500\u25ba results_c.json \u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/parallel/#what-it-does","title":"What It Does","text":"<p>This pattern is common in research: generate data once, then run the same analysis with different hyperparameters to find the best configuration.</p> <ol> <li>generate_data: Creates synthetic classification data</li> <li>process_config_a/b/c: Three parallel runs with different thresholds</li> <li>compare_results: Aggregates all results into a comparison table</li> </ol> <p>Each \"process\" step uses a different threshold parameter, producing separate outputs.</p>"},{"location":"tutorials/parallel/#run-it","title":"Run It","text":"<pre><code># Run full pipeline (parallel execution enabled - configs run concurrently)\nloom examples/parallel/pipeline.yml\n\n# Run sequentially instead\nloom examples/parallel/pipeline.yml --sequential\n\n# Check outputs\ncat examples/parallel/data/dataset.csv           # Generated data\ncat examples/parallel/data/results_low.json      # Threshold 0.3\ncat examples/parallel/data/results_mid.json      # Threshold 0.5\ncat examples/parallel/data/results_high.json     # Threshold 0.7\ncat examples/parallel/data/comparison.json       # Side-by-side comparison\n\n# Run just one configuration branch\nloom examples/parallel/pipeline.yml --step process_config_low\n\n# Open in editor to see the parallel structure\nloom-ui examples/parallel/pipeline.yml\n</code></pre>"},{"location":"tutorials/parallel/#parallel-execution","title":"Parallel Execution","text":"<p>This pipeline uses parallel execution (configured in <code>pipeline.yml</code>):</p> <pre><code>execution:\n  parallel: true\n  max_workers: 3\n</code></pre> <p>With this configuration, the three <code>process_config_*</code> steps run concurrently after <code>generate_data</code> completes. The <code>compare_results</code> step waits for all three to finish before running.</p> <p>Note: The <code>process.py</code> task includes a random 2-5 second delay to make parallel execution visible. When running in parallel, all three process steps start together and complete in ~5 seconds total. Running sequentially (<code>--sequential</code>) takes ~12 seconds since each step waits for the previous one.</p>"},{"location":"tutorials/parallel/#pattern-hyperparameter-search","title":"Pattern: Hyperparameter Search","text":"<p>The key pattern here is: 1. One input source feeds multiple processing branches 2. Each branch has different parameters but the same task script 3. Results are aggregated at the end for comparison</p> <p>This is useful for: - Trying different model hyperparameters - Comparing algorithm variants - A/B testing processing approaches</p>"},{"location":"tutorials/parallel/#extending-this-pattern","title":"Extending This Pattern","text":"<p>To add more configurations: 1. Add a new variable for the output: <code>results_d: data/results_d.json</code> 2. Copy one of the <code>process_config_*</code> steps, rename it, change the threshold 3. Add the new result to <code>compare_results</code> inputs</p>"},{"location":"tutorials/parallel/#files","title":"Files","text":"<ul> <li><code>pipeline.yml</code> \u2014 Pipeline with parallel branches</li> <li><code>tasks/generate_data.py</code> \u2014 Creates synthetic data</li> <li><code>tasks/process.py</code> \u2014 Classifies data with configurable threshold</li> <li><code>tasks/compare_results.py</code> \u2014 Aggregates results for comparison</li> </ul>"},{"location":"tutorials/parameters/","title":"Parameters Example","text":"<p>Using parameters for configuration and overriding them at runtime with <code>--set</code>.</p>"},{"location":"tutorials/parameters/#what-it-does","title":"What It Does","text":"<p>A simple signal processing pipeline that: 1. generate_signal: Creates a noisy sine wave 2. smooth_signal: Applies a moving average filter 3. detect_peaks: Finds peaks above a threshold</p> <p>Parameters control the signal characteristics and processing settings.</p>"},{"location":"tutorials/parameters/#run-it","title":"Run It","text":"<pre><code># Run with default parameters\nloom examples/parameters/pipeline.yml\n\n# Check outputs\ncat examples/parameters/data/signal.csv         # Raw noisy signal\ncat examples/parameters/data/smoothed.csv       # After smoothing\ncat examples/parameters/data/peaks.json         # Detected peaks\n\n# Override parameters at runtime\nloom examples/parameters/pipeline.yml --set window_size=10 threshold=0.5\n\n# Try different noise levels\nloom examples/parameters/pipeline.yml --set noise_level=0.5\n\n# Preview what would run\nloom examples/parameters/pipeline.yml --set window_size=20 --dry-run\n</code></pre>"},{"location":"tutorials/parameters/#parameters","title":"Parameters","text":"Parameter Default Description <code>num_points</code> 200 Number of data points to generate <code>noise_level</code> 0.2 Amount of random noise (0-1) <code>frequency</code> 2.0 Signal frequency (cycles per series) <code>window_size</code> 5 Moving average window size <code>threshold</code> 0.3 Peak detection threshold"},{"location":"tutorials/parameters/#files","title":"Files","text":"<ul> <li><code>pipeline.yml</code> \u2014 Pipeline with parameterized configuration</li> <li><code>tasks/generate_signal.py</code> \u2014 Generates noisy sine wave</li> <li><code>tasks/smooth_signal.py</code> \u2014 Moving average smoothing</li> <li><code>tasks/detect_peaks.py</code> \u2014 Peak detection</li> </ul>"}]}